{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XCtmPf3saRr"
   },
   "source": [
    "### Problem statement\n",
    "\n",
    "3*3 grid \\\n",
    "Indexing 0-8 \n",
    "\n",
    "\n",
    "Fish (+1 reward) on 4,6\\\n",
    "Death (-10 reward) and termination on 5 \\\n",
    "Finish and +10 reward on 8 \\\n",
    "-0.1 reward (can change later if we get no termination) for each step taken. \n",
    "\n",
    "\n",
    "Start at 0 \\\n",
    "4 actions: u,d,l,r  \\\n",
    "Each action has 0.25 prob initially.  \n",
    "\n",
    "\n",
    "After action is decided, 0.5 prob that we take one step in that direction and 0.5 that we slip and take 2 steps in that  direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "juAWNL8hsaRt",
    "outputId": "63ca70c7-37ab-4455-d646-fa0064d58491"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "# Difference between forward and backward T -D learning\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEtEglw0saRv"
   },
   "source": [
    "### Environment Creation of Given respective game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zqsCILqNsaRv"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Action:\n",
    "    def __init__(self):\n",
    "        self.L=0\n",
    "        self.R=1\n",
    "        self.U=2\n",
    "        self.D=3\n",
    "    def check(self):\n",
    "        pass\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.action_space=4\n",
    "        self.observation_space=9\n",
    "        self.state=1\n",
    "        self.done=False\n",
    "        self.reward=[[0,0,0],[0,1,-10],[1,0,10]]\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state=0\n",
    "        return self.state\n",
    "        \n",
    "    def sample(self):\n",
    "        return np.random.choice([0,1,2,3],p=[0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.Reward=0\n",
    "        Act=Action()\n",
    "        if(self.state==0):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([1,2],p=[0.5,0.5])\n",
    "                if(self.state==2):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([1,2],p=[0.5,0.5])\n",
    "                if(self.state==2):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([3,6],p=[0.5,0.5])\n",
    "                if self.state==3:\n",
    "                    self.Reward=self.reward[1][0] -0.2\n",
    "                else :\n",
    "                    self.Reward=self.reward[2][0] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([3,6],p=[0.5,0.5])\n",
    "                if self.state==3:\n",
    "                    self.Reward=self.reward[1][0] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[2][0] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        elif(self.state==1):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([0,2],p=[0.5,0.5])\n",
    "                if(self.state==2):\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([0,2],p=[0.5,0.5])\n",
    "                if(self.state==2):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([4,7],p=[0.5,0.5])\n",
    "                if self.state==4:\n",
    "                    self.Reward=self.reward[1][1] -0.2\n",
    "                else :\n",
    "                    self.Reward=self.reward[2][1] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([4,7],p=[0.5,0.5])\n",
    "                if self.state==4:\n",
    "                    self.Reward=self.reward[1][1] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[2][1] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        elif(self.state==2):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([1,0],p=[0.5,0.5])\n",
    "                if(self.state==1):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([1,0],p=[0.5,0.5])\n",
    "                if(self.state==0):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([5,8],p=[0.5,0.5])\n",
    "                if self.state==5:\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                else :\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                self.done=True\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([5,8],p=[0.5,0.5])\n",
    "                if self.state==5:\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                else :\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                self.done=True\n",
    "                \n",
    "        elif(self.state==3):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([5,4],p=[0.5,0.5])\n",
    "                if(self.state==5):\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                    self.done=True\n",
    "                else:\n",
    "                    self.Reward=self.reward[1][1] -0.2\n",
    "                    self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([5,4],p=[0.5,0.5])\n",
    "                if(self.state==4):\n",
    "                    self.Reward=self.reward[1][1] -0.1\n",
    "                    self.done=False\n",
    "                else:\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                    self.done=True\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([0,6],p=[0.5,0.5])\n",
    "                if self.state==6:\n",
    "                    self.Reward=self.reward[2][0] -0.2\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][0] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([0,6],p=[0.5,0.5])\n",
    "                if self.state==6:\n",
    "                    self.Reward=self.reward[2][0] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][0] -0.2\n",
    "                self.done=False\n",
    "                   \n",
    "        elif(self.state==4):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([5,3],p=[0.5,0.5])\n",
    "                if(self.state==5):\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                    self.done=True\n",
    "                else:\n",
    "                    self.Reward=self.reward[1][0] -0.2\n",
    "                    self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([5,3],p=[0.5,0.5])\n",
    "                if(self.state==3):\n",
    "                    self.Reward=self.reward[1][0] -0.1\n",
    "                    self.done=False\n",
    "                else:\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                    self.done=True\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([1,7],p=[0.5,0.5])\n",
    "                if self.state==7:\n",
    "                    self.Reward=self.reward[2][1] -0.2\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][1] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([1,7],p=[0.5,0.5])\n",
    "                if self.state==7:\n",
    "                    self.Reward=self.reward[2][1] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][1] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        elif(self.state==6):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([7,8],p=[0.5,0.5])\n",
    "                if(self.state==8):\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                    self.done=True\n",
    "                else:\n",
    "                    self.Reward=self.reward[2][1] -0.2\n",
    "                    self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([7,8],p=[0.5,0.5])\n",
    "                if(self.state==7):\n",
    "                    self.Reward=self.reward[2][1] -0.1\n",
    "                    self.done=False\n",
    "                else:\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                    self.done=True\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([0,3],p=[0.5,0.5])\n",
    "                if self.state==3:\n",
    "                    self.Reward=self.reward[1][0] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][0] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([0,3],p=[0.5,0.5])\n",
    "                if self.state==0:\n",
    "                    self.Reward=self.reward[0][0] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[1][0] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        elif(self.state==7):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([6,8],p=[0.5,0.5])\n",
    "                if(self.state==6):\n",
    "                    self.Reward=self.reward[2][0] -0.1\n",
    "                    self.done=False\n",
    "                else:\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                    self.done=True\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([6,8],p=[0.5,0.5])\n",
    "                if(self.state==8):\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                    self.done=True\n",
    "                else:\n",
    "                    self.Reward=self.reward[2][0] -0.2\n",
    "                    self.done=False\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([1,4],p=[0.5,0.5])\n",
    "                if self.state==4:\n",
    "                    self.Reward=self.reward[1][1] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][1] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([1,4],p=[0.5,0.5])\n",
    "                if self.state==0:\n",
    "                    self.Reward=self.reward[0][1] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[1][1] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        return self.state,self.Reward,self.done\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08uZJNy0saRy"
   },
   "source": [
    "### Human Playing game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LD4ChGosaRy",
    "outputId": "2a5c4807-86ed-41f5-ced5-37df84bfe5f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->0\n",
      "Your new state is  2\n",
      "Your current reward is -0.1\n",
      "Your Total reward is -0.1\n",
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->1\n",
      "Your new state is  1\n",
      "Your current reward is -0.2\n",
      "Your Total reward is -0.30000000000000004\n",
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->2\n",
      "Your new state is  7\n",
      "Your current reward is -0.1\n",
      "Your Total reward is -0.4\n",
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->1\n",
      "Your new state is  6\n",
      "Your current reward is 0.8\n",
      "Your Total reward is 0.4\n",
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->1\n",
      "Your new state is  8\n",
      "Your current reward is 10\n",
      "Your Total reward is 10.4\n",
      "You entered the termination\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "while True:\n",
    "    action = int(input(\"Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->\"))\n",
    "    state,reward,done = env.step(action)\n",
    "    total_reward  += reward;\n",
    "    print(\"Your new state is \", state)\n",
    "    print(\"Your current reward is\", reward )\n",
    "    print(\"Your Total reward is\", total_reward )\n",
    "    if(done):\n",
    "        print(\"You entered the termination\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMlxTh9lsaRz"
   },
   "source": [
    "### Monte-Carlo Control Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e8hGxKzAsaRz"
   },
   "outputs": [],
   "source": [
    "class MC_agent:\n",
    "    def __init__(self,enviroment):\n",
    "        \n",
    "        self.num_episodes=6000\n",
    "        \n",
    "        self.max_steps_per_episode = 20000\n",
    "        \n",
    "        #self.n0 = float(n0)\n",
    "        \n",
    "        self.env = enviroment\n",
    "        \n",
    "        self.rewards_all_episodes = []\n",
    "\n",
    "        # N(s,a) is the number of times that action a has been selected from state s.\n",
    "        self.N = np.zeros((self.env.observation_space,self.env.action_space))\n",
    "        \n",
    "        # Q-table for Q(s,a)\n",
    "        self.Q = np.zeros((self.env.observation_space,self.env.action_space))\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        # Loop episodes\n",
    "        for episode in range(1,self.num_episodes+1):\n",
    "            \n",
    "            \n",
    "            #for storing each state, action pair in each step of episodes\n",
    "            episode_pairs = []\n",
    "            \n",
    "            # get initial state for current episode\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            rewards_current_episode = 0\n",
    "            \n",
    "            done = False\n",
    "            \n",
    "            for step in range(self.max_steps_per_episode):\n",
    "                \n",
    "                # get optimal action, with epsilon exploration (epsilon dependent on number of visits to the state)\n",
    "                # ε-greedy exploration strategy with εt = N0/(N0 + N(st))\n",
    "                # n_visits = sum(self.N[state, :])\n",
    "                # epsilon = N0/(N0 + N(st))\n",
    "                #curr_epsilon = self.n0 / (self.n0 + n_visits)\n",
    "                \n",
    "                # we are using this curr_epsilon=1/k  where k=episode\n",
    "                curr_epsilon=1/episode\n",
    "\n",
    "                # epsilon greedy policy exploration\n",
    "                if np.random.uniform(0, 1) < curr_epsilon:\n",
    "                    action = self.env.sample()\n",
    "                    \n",
    "                else:\n",
    "                    action = np.argmax(self.Q[state,:]) \n",
    "               \n",
    "                # store action state pairs\n",
    "                episode_pairs.append((state,action))\n",
    "                \n",
    "                # update visits\n",
    "                # N(s,a) is the number of times that action a has been selected from state s. \n",
    "                self.N[state, action] += 1\n",
    "                \n",
    "                # execute action\n",
    "                state,reward,done = self.env.step(action)\n",
    "                \n",
    "                rewards_current_episode += reward\n",
    "                \n",
    "                # when game ends\n",
    "                if(done == True):\n",
    "                    break\n",
    "             \n",
    "            self.rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "            # Update Action value function accordingly\n",
    "            for curr_state, curr_action in episode_pairs:\n",
    "                \n",
    "                # Alpha(learning rate)\n",
    "                step = 1.0 / self.N[curr_state, curr_action]   \n",
    "            \n",
    "                error = rewards_current_episode - self.Q[curr_state, curr_action]\n",
    "                \n",
    "                self.Q[curr_state, curr_action] += step * error\n",
    "                \n",
    "    def showing_and_storing_q_val(self):\n",
    "        \n",
    "        print(\"\\n\\n********Q-table********\\n\")\n",
    "        print(self.Q)\n",
    "        #Saving Q table\n",
    "        np.save('q_table_for_monti_carlo',self.Q)\n",
    "        \n",
    "    def accuracy(self):\n",
    "        rewards_per_thousand_episodes = np.split(np.array(self.rewards_all_episodes),self.num_episodes/1000)\n",
    "        count = 1000\n",
    "\n",
    "        print(\"********Average reward per thousand episodes********\\n\")\n",
    "        for r in rewards_per_thousand_episodes:\n",
    "            print(count, \": \", str(sum(r/1000)))\n",
    "            count += 1000\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E62M0DmesaR0",
    "outputId": "a6602722-d1cc-4d96-cf8f-efec51422e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[2546.23115952 3144.0354303  2791.88778153 3350.47641501]\n",
      " [2947.9573761  2198.73646393 4595.20215581 3905.35059604]\n",
      " [ -16.98834019 3268.69494446  526.05        -11.        ]\n",
      " [2285.88244889 2294.66634662 3350.79835616 2600.88531565]\n",
      " [2784.92454523 2829.22905799 2649.49499962 4595.43174236]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [2065.55146866 2289.23689916 2774.39425894 3351.17752884]\n",
      " [2757.04223053 2861.46758641 3687.26073468 4595.19173888]\n",
      " [   0.            0.            0.            0.        ]]\n",
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  719.7522000000271\n",
      "2000 :  1939.3215000002372\n",
      "3000 :  2516.077200000405\n",
      "4000 :  2978.4487000005242\n",
      "5000 :  3109.1748000005928\n",
      "6000 :  3391.3052000006855\n"
     ]
    }
   ],
   "source": [
    "agent = MC_agent(Environment())\n",
    "agent.train()\n",
    "agent.showing_and_storing_q_val()\n",
    "agent.accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRp-OOSRsaR0"
   },
   "source": [
    "### The Code To Watch The Agent Play The Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9K9W5SisaR1"
   },
   "outputs": [],
   "source": [
    "q_table=np.load('q_table_for_monti_carlo.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SFWj98Ps4R5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env= Environment()\n",
    "for episode in range(1):\n",
    "    Total_reward=0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\")\n",
    "    while True:        \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done= env.step(action)\n",
    "        if(action==0):\n",
    "            print(\"You Took action in direction Left Ward 🠔\")\n",
    "            \n",
    "        elif(action==1):\n",
    "            print(\"You Took action in direction Right Ward  ➜ \")\n",
    "            \n",
    "        elif(action==3):\n",
    "            print(\"You Took action in direction Upward Ward 🠕\")\n",
    "            \n",
    "        else:\n",
    "            print(\"You Took action in direction Down Ward 🠗 \")\n",
    "            \n",
    "            \n",
    "        print(\"You are moving \",state,\"->\",new_state,\"\\n\")\n",
    "        \n",
    "        Total_reward=Total_reward+reward\n",
    "        if done:\n",
    "            if new_state == 8:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                print(\"Total Reward  of EPISODE \",episode+1,\" is \",Total_reward,\"\\n\")\n",
    "\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                print(\"Total Reward  of EPISODE \",episode+1,\" is \",Total_reward,\"\\n\")\n",
    "\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_JFvDGysaca"
   },
   "source": [
    "### T-D( lambada) Control Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-ys_52Efsaci"
   },
   "outputs": [],
   "source": [
    "class Sarsa_Agent:\n",
    "    \n",
    "    def __init__(self, environment, mlambda):\n",
    "        #during chossing appropriate  λ value\n",
    "        #self.num_episodes=1000\n",
    "        \n",
    "        # For best  λ carring agent\n",
    "        self.num_episodes=6000\n",
    "        self.max_steps_per_episode = 20000\n",
    "        #self.n0 = float(n0)\n",
    "        \n",
    "        self.env = environment\n",
    "        self.mlambda = mlambda\n",
    "        \n",
    "        # N(s) is the number of times that state s has been visited\n",
    "        # N(s,a) is the number of times that action a has been selected from state s.\n",
    "        self.N = np.zeros((self.env.observation_space,\n",
    "                           self.env.action_space))\n",
    "        \n",
    "        self.Q = np.zeros((self.env.observation_space,\n",
    "                           self.env.action_space))\n",
    "        \n",
    "        self.rewards_all_episodes = []\n",
    "        \n",
    "\n",
    "    # get optimal action, with epsilon exploration (epsilon dependent on number of visits to the state)\n",
    "    # ε-greedy exploration strategy with εt = N0/(N0 + N(st)), \n",
    "    def train_get_action(self, state,episode):\n",
    "\n",
    "        #n_visits = sum(self.N[state, :])       \n",
    "\n",
    "        # epsilon = N0/(N0 + N(st))\n",
    "        #curr_epsilon = self.n0 / (self.n0 + n_visits)\n",
    "        \n",
    "        # we are using this curr_epsilon=1/k  where k=episode\n",
    "        curr_epsilon = 1/episode\n",
    "        # epsilon greedy policy\n",
    "        if np.random.uniform(0, 1) < curr_epsilon:\n",
    "            action = self.env.sample()\n",
    "            return action\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state, :])\n",
    "            return action\n",
    "\n",
    "\n",
    "    def train(self):        \n",
    "        \n",
    "        # Loop episodes\n",
    "        for episode in range(1,self.num_episodes+1):\n",
    "            \n",
    "            self.E =np.zeros((self.env.observation_space, self.env.action_space))\n",
    "            \n",
    "            self.current_reward=0\n",
    "            # get initial state for current episode\n",
    "            state = self.env.reset()\n",
    "            action = self.train_get_action(state,episode)\n",
    "            action_next = action\n",
    "            done=False\n",
    "            # Execute until game ends\n",
    "            for step in range(self.max_steps_per_episode):\n",
    "                # update visits\n",
    "                \n",
    "                self.N[state, action] += 1\n",
    "                \n",
    "                # execute action\n",
    "                state_next, reward,done = self.env.step(action)\n",
    "                \n",
    "                self.current_reward+=reward\n",
    "                q_value = self.Q[state, action]\n",
    "                                \n",
    "                if not done:\n",
    "                    # choose next action with epsilon greedy policy\n",
    "                    action_next = self.train_get_action(state_next,episode)\n",
    "                    \n",
    "                    next_q_value= self.Q[state_next, action_next]\n",
    "                    delta = reward + next_q_value - q_value\n",
    "                else:\n",
    "                    delta = reward - q_value\n",
    "\n",
    "                \n",
    "                self.E[state, action] += 1\n",
    "                alpha = 1.0  / (self.N[state, action])\n",
    "                \n",
    "                update = alpha * delta * self.E\n",
    "                self.Q += update\n",
    "                self.E *= self.mlambda\n",
    "\n",
    "                # reassign s and a\n",
    "                state = state_next\n",
    "                action =  action_next\n",
    "                \n",
    "                if(done==True):\n",
    "                    break\n",
    "            \n",
    "            self.rewards_all_episodes.append(self.current_reward) \n",
    "\n",
    "    def showing_and_storing_q_val(self):\n",
    "        \n",
    "        print(\"\\n\\n********Q-table********\\n\")\n",
    "        print(self.Q)\n",
    "        #Saving Q table\n",
    "        np.save('q_table_for_TD_learning',self.Q)\n",
    "        \n",
    "    def accuracy(self):\n",
    "        rewards_per_thousand_episodes = np.split(np.array(self.rewards_all_episodes),self.num_episodes/1000)\n",
    "        count = 1000\n",
    "\n",
    "        print(\"********Average reward per thousand episodes********\\n\")\n",
    "        for r in rewards_per_thousand_episodes:\n",
    "            print(count, \": \", str(sum(r/1000)))\n",
    "            count += 1000\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCF6C67gsacj"
   },
   "source": [
    "## Choosing best λ On the basis of mean- squared error.\n",
    "**Stop each run after 1000 episodes and report the best Q(s,a)on the basis of  mean-squared error over all states and actions with the given respective lambda, comparing the true values Q∗(s,a)(Monti Carlo computed in the previous section with the estimated values Q(s, a) computed by Sarsa.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wmkXVRGHsack"
   },
   "outputs": [],
   "source": [
    "mc_agent_Q_table=np.load('q_table_for_monti_carlo.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ypsbpj2rsack"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d0432a8ec8>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "lambdas = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "agent_list = []\n",
    "sme_list = []\n",
    "n_elements = mc_agent_Q_table.shape[0]*mc_agent_Q_table.shape[1]*2\n",
    "for l in lambdas:\n",
    "    agent = Sarsa_Agent(Environment(),l)\n",
    "    agent_list.append(l)\n",
    "    agent.train()\n",
    "    sme = np.sum(np.square(agent.Q-mc_agent_Q_table))/float(n_elements)\n",
    "    sme_list.append(sme)\n",
    "\n",
    "plt.title(\"λ vs MSE in Saras\")\n",
    "plt.plot(agent_list,sme_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### So the appropriate lambda value is less than 0.7 lets choose 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Jj9oH9ytsacl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[-2.11490025e+00  3.64739840e+03  0.00000000e+00 -6.54052323e-01]\n",
      " [ 1.65215781e+03  1.58339230e+03  1.08246054e+04  5.48897453e+03]\n",
      " [ 3.60474362e+00  3.26627717e+03  2.55813953e+00 -1.00000000e+01]\n",
      " [-1.00000000e+01 -1.00000000e+01  1.25357935e+03  0.00000000e+00]\n",
      " [ 4.58200317e+02  4.72757455e+02  5.48497502e+03  1.08269072e+04]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.39054473e+03  1.26595164e+01  6.93300520e+00  8.48882580e+02]\n",
      " [ 8.54490120e+02  3.29368484e+02  5.37760641e+03  1.09576395e+04]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  10.113500000000037\n",
      "2000 :  10.532100000000053\n",
      "3000 :  2473.1994000002956\n",
      "4000 :  3750.7480000004693\n",
      "5000 :  4237.569400000548\n",
      "6000 :  4435.703500000569\n"
     ]
    }
   ],
   "source": [
    "lambda_value = 0.6\n",
    "agent =Sarsa_Agent(Environment(),lambda_value)\n",
    "agent.train()\n",
    "agent.showing_and_storing_q_val()\n",
    "agent.accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-y8a9mEsacl"
   },
   "source": [
    "### The Code To Watch The Agent Play The Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_PpQj9EKsacl"
   },
   "outputs": [],
   "source": [
    "q_table=np.load('q_table_for_TD_learning.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obuDrICLsacm"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env= Environment()\n",
    "for episode in range(9):\n",
    "    Total_reward=0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\")\n",
    "    while True:        \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done= env.step(action)\n",
    "        if(action==0):\n",
    "            print(\"You Took action in direction Left Ward 🠔\")\n",
    "            \n",
    "        elif(action==1):\n",
    "            print(\"You Took action in direction Right Ward  ➜ \")\n",
    "            \n",
    "        elif(action==3):\n",
    "            print(\"You Took action in direction Upward Ward 🠕\")\n",
    "            \n",
    "        else:\n",
    "            print(\"You Took action in direction Down Ward 🠗 \")\n",
    "            \n",
    "            \n",
    "        print(\"You are moving \",state,\"->\",new_state,\"\\n\")\n",
    "        \n",
    "        Total_reward=Total_reward+reward\n",
    "        if done:\n",
    "            if new_state == 8:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                print(\"Total Reward  of EPISODE \",episode+1,\" is \",Total_reward,\"\\n\")\n",
    "\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                print(\"Total Reward  of EPISODE \",episode+1,\" is \",Total_reward,\"\\n\")\n",
    "\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ice-word_MC_TD(lambda).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
