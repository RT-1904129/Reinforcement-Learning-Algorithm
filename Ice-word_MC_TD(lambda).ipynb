{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "\n",
    "3*3 grid \\\n",
    "Indexing 0-8 \n",
    "\n",
    "\n",
    "Fish (+1 reward) on 4,6\\\n",
    "Death (-10 reward) and termination on 5 \\\n",
    "Finish and +10 reward on 8 \\\n",
    "-0.1 reward (can change later if we get no termination) for each step taken. \n",
    "\n",
    "\n",
    "Start at 0 \\\n",
    "4 actions: u,d,l,r  \\\n",
    "Each action has 0.25 prob initially.  \n",
    "\n",
    "\n",
    "After action is decided, 0.5 prob that we take one step in that direction and 0.5 that we slip and take 2 steps in that  direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "# Difference between forward and backward T -D learning\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Creation of Given respective game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Action:\n",
    "    def __init__(self):\n",
    "        self.L=0\n",
    "        self.R=1\n",
    "        self.U=2\n",
    "        self.D=3\n",
    "    def check(self):\n",
    "        pass\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.action_space=4\n",
    "        self.observation_space=9\n",
    "        self.state=1\n",
    "        self.done=False\n",
    "        self.reward=[[0,0,0],[0,1,-10],[1,0,10]]\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state=0\n",
    "        return self.state\n",
    "        \n",
    "    def sample(self):\n",
    "        return np.random.choice([0,1,2,3],p=[0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.Reward=0\n",
    "        Act=Action()\n",
    "        if(self.state==0):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([1,2],p=[0.5,0.5])\n",
    "                if(self.state==2):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([1,2],p=[0.5,0.5])\n",
    "                if(self.state==2):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([3,6],p=[0.5,0.5])\n",
    "                if self.state==3:\n",
    "                    self.Reward=self.reward[1][0] -0.2\n",
    "                else :\n",
    "                    self.Reward=self.reward[2][0] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([3,6],p=[0.5,0.5])\n",
    "                if self.state==3:\n",
    "                    self.Reward=self.reward[1][0] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[2][0] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        elif(self.state==1):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([0,2],p=[0.5,0.5])\n",
    "                if(self.state==2):\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([0,2],p=[0.5,0.5])\n",
    "                if(self.state==2):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([4,7],p=[0.5,0.5])\n",
    "                if self.state==4:\n",
    "                    self.Reward=self.reward[1][1] -0.2\n",
    "                else :\n",
    "                    self.Reward=self.reward[2][1] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([4,7],p=[0.5,0.5])\n",
    "                if self.state==4:\n",
    "                    self.Reward=self.reward[1][1] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[2][1] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        elif(self.state==2):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([1,0],p=[0.5,0.5])\n",
    "                if(self.state==1):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([1,0],p=[0.5,0.5])\n",
    "                if(self.state==0):\n",
    "                    self.Reward=self.reward[0][self.state] -0.1\n",
    "                else:\n",
    "                    self.Reward=self.reward[0][self.state] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([5,8],p=[0.5,0.5])\n",
    "                if self.state==5:\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                else :\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                self.done=True\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([5,8],p=[0.5,0.5])\n",
    "                if self.state==5:\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                else :\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                self.done=True\n",
    "                \n",
    "        elif(self.state==3):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([5,4],p=[0.5,0.5])\n",
    "                if(self.state==5):\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                    self.done=True\n",
    "                else:\n",
    "                    self.Reward=self.reward[1][1] -0.2\n",
    "                    self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([5,4],p=[0.5,0.5])\n",
    "                if(self.state==4):\n",
    "                    self.Reward=self.reward[1][1] -0.1\n",
    "                    self.done=False\n",
    "                else:\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                    self.done=True\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([0,6],p=[0.5,0.5])\n",
    "                if self.state==6:\n",
    "                    self.Reward=self.reward[2][0] -0.2\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][0] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([0,6],p=[0.5,0.5])\n",
    "                if self.state==6:\n",
    "                    self.Reward=self.reward[2][0] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][0] -0.2\n",
    "                self.done=False\n",
    "                   \n",
    "        elif(self.state==4):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([5,3],p=[0.5,0.5])\n",
    "                if(self.state==5):\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                    self.done=True\n",
    "                else:\n",
    "                    self.Reward=self.reward[1][0] -0.2\n",
    "                    self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([5,3],p=[0.5,0.5])\n",
    "                if(self.state==3):\n",
    "                    self.Reward=self.reward[1][0] -0.1\n",
    "                    self.done=False\n",
    "                else:\n",
    "                    self.Reward=self.reward[1][2] \n",
    "                    self.done=True\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([1,7],p=[0.5,0.5])\n",
    "                if self.state==7:\n",
    "                    self.Reward=self.reward[2][1] -0.2\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][1] -0.1\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([1,7],p=[0.5,0.5])\n",
    "                if self.state==7:\n",
    "                    self.Reward=self.reward[2][1] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][1] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        elif(self.state==6):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([7,8],p=[0.5,0.5])\n",
    "                if(self.state==8):\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                    self.done=True\n",
    "                else:\n",
    "                    self.Reward=self.reward[2][1] -0.2\n",
    "                    self.done=False\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([7,8],p=[0.5,0.5])\n",
    "                if(self.state==7):\n",
    "                    self.Reward=self.reward[2][1] -0.1\n",
    "                    self.done=False\n",
    "                else:\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                    self.done=True\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([0,3],p=[0.5,0.5])\n",
    "                if self.state==3:\n",
    "                    self.Reward=self.reward[1][0] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][0] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([0,3],p=[0.5,0.5])\n",
    "                if self.state==0:\n",
    "                    self.Reward=self.reward[0][0] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[1][0] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        elif(self.state==7):\n",
    "            if(action==Act.L):\n",
    "                self.state=np.random.choice([6,8],p=[0.5,0.5])\n",
    "                if(self.state==6):\n",
    "                    self.Reward=self.reward[2][0] -0.1\n",
    "                    self.done=False\n",
    "                else:\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                    self.done=True\n",
    "                \n",
    "            elif (action==Act.R):\n",
    "                self.state=np.random.choice([6,8],p=[0.5,0.5])\n",
    "                if(self.state==8):\n",
    "                    self.Reward=self.reward[2][2] \n",
    "                    self.done=True\n",
    "                else:\n",
    "                    self.Reward=self.reward[2][0] -0.2\n",
    "                    self.done=False\n",
    "                \n",
    "                \n",
    "            elif(action==Act.U):\n",
    "                self.state=np.random.choice([1,4],p=[0.5,0.5])\n",
    "                if self.state==4:\n",
    "                    self.Reward=self.reward[1][1] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[0][1] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "            elif(action==Act.D):\n",
    "                self.state=np.random.choice([1,4],p=[0.5,0.5])\n",
    "                if self.state==0:\n",
    "                    self.Reward=self.reward[0][1] -0.1\n",
    "                else :\n",
    "                    self.Reward=self.reward[1][1] -0.2\n",
    "                self.done=False\n",
    "                \n",
    "        return self.state,self.Reward,self.done\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Playing game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->2\n",
      "Your new state is  6\n",
      "Your current reward is 0.9\n",
      "Your Total reward is 0.9\n",
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->0\n",
      "Your new state is  7\n",
      "Your current reward is -0.2\n",
      "Your Total reward is 0.7\n",
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->0\n",
      "Your new state is  6\n",
      "Your current reward is 0.9\n",
      "Your Total reward is 1.6\n",
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->1\n",
      "Your new state is  7\n",
      "Your current reward is -0.1\n",
      "Your Total reward is 1.5\n",
      "Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->1\n",
      "Your new state is  8\n",
      "Your current reward is 0\n",
      "Your Total reward is 1.5\n",
      "You entered the termination\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "while True:\n",
    "    action = int(input(\"Enter a direction: 0 (LEFT), 1 (RIGHT), 2 (UP), 3(DOWN) ->\"))\n",
    "    state,reward,done = env.step(action)\n",
    "    total_reward  += reward;\n",
    "    print(\"Your new state is \", state)\n",
    "    print(\"Your current reward is\", reward )\n",
    "    print(\"Your Total reward is\", total_reward )\n",
    "    if(done):\n",
    "        print(\"You entered the termination\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo Control Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_agent:\n",
    "    def __init__(self,enviroment,n0):\n",
    "        \n",
    "        self.num_episodes=30000\n",
    "        \n",
    "        self.max_steps_per_episode = 3000\n",
    "        \n",
    "        self.n0 = float(n0)\n",
    "        \n",
    "        self.env = enviroment\n",
    "        \n",
    "        self.rewards_all_episodes = []\n",
    "\n",
    "        # N(s,a) is the number of times that action a has been selected from state s.\n",
    "        self.N = np.zeros((self.env.observation_space,self.env.action_space))\n",
    "        \n",
    "        # Q-table for Q(s,a)\n",
    "        self.Q = np.zeros((self.env.observation_space,self.env.action_space))\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        # Loop episodes\n",
    "        for episode in range(self.num_episodes):\n",
    "            \n",
    "            \n",
    "            #for storing each state, action pair in each step of episodes\n",
    "            episode_pairs = []\n",
    "            \n",
    "            # get initial state for current episode\n",
    "            state = self.env.reset()\n",
    "            \n",
    "            rewards_current_episode = 0\n",
    "            \n",
    "            done = False\n",
    "            \n",
    "            for step in range(self.max_steps_per_episode):\n",
    "                \n",
    "                # get optimal action, with epsilon exploration (epsilon dependent on number of visits to the state)\n",
    "                # ε-greedy exploration strategy with εt = N0/(N0 + N(st))\n",
    "                n_visits = sum(self.N[state, :])\n",
    "\n",
    "                # epsilon = N0/(N0 + N(st))\n",
    "                curr_epsilon = self.n0 / (self.n0 + n_visits)\n",
    "\n",
    "                # epsilon greedy policy exploration\n",
    "                if np.random.uniform(0, 1) < curr_epsilon:\n",
    "                    action = self.env.sample()\n",
    "                    \n",
    "                else:\n",
    "                    action = np.argmax(self.Q[state,:]) \n",
    "               \n",
    "                # store action state pairs\n",
    "                episode_pairs.append((state,action))\n",
    "                \n",
    "                # update visits\n",
    "                # N(s,a) is the number of times that action a has been selected from state s. \n",
    "                self.N[state, action] += 1\n",
    "                \n",
    "                # execute action\n",
    "                state,reward,done = self.env.step(action)\n",
    "                \n",
    "                rewards_current_episode += reward\n",
    "                \n",
    "                # when game ends\n",
    "                if(done == True):\n",
    "                    break\n",
    "             \n",
    "            self.rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "            # Update Action value function accordingly\n",
    "            for curr_state, curr_action in episode_pairs:\n",
    "                \n",
    "                # Alpha(learning rate)\n",
    "                step = 1.0 / self.N[curr_state, curr_action]   \n",
    "            \n",
    "                error = rewards_current_episode - self.Q[curr_state, curr_action]\n",
    "                \n",
    "                self.Q[curr_state, curr_action] += step * error\n",
    "                \n",
    "    def showing_and_storing_q_val(self):\n",
    "        \n",
    "        print(\"\\n\\n********Q-table********\\n\")\n",
    "        print(self.Q)\n",
    "        #Saving Q table\n",
    "        np.save('q_table_for_monti_carlo',self.Q)\n",
    "        \n",
    "    def accuracy(self):\n",
    "        rewards_per_thousand_episodes = np.split(np.array(self.rewards_all_episodes),self.num_episodes/1000)\n",
    "        count = 1000\n",
    "\n",
    "        print(\"********Average reward per thousand episodes********\\n\")\n",
    "        for r in rewards_per_thousand_episodes:\n",
    "            print(count, \": \", str(sum(r/1000)))\n",
    "            count += 1000\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[ 6.05277112  6.24479396 10.16044457  7.84039968]\n",
      " [ 6.4074335   6.71654378  9.16955346  7.05470563]\n",
      " [ 7.85099041  6.71993433 -0.85435897 -1.03168317]\n",
      " [-1.28939081 -2.42446643  8.87954415 10.24034258]\n",
      " [-0.79009889 -0.77927057  8.84455554 10.05736453]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [10.43243068 11.1439657   8.87250883  8.95086004]\n",
      " [11.38172881 10.85274878  7.67012474  8.42492678]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  3.685999999999993\n",
      "2000 :  7.5199000000000105\n",
      "3000 :  8.93950000000001\n",
      "4000 :  9.175599999999998\n",
      "5000 :  9.267100000000006\n",
      "6000 :  10.033899999999985\n",
      "7000 :  10.173299999999992\n",
      "8000 :  10.184700000000003\n",
      "9000 :  10.357899999999985\n",
      "10000 :  10.30239999999998\n",
      "11000 :  10.424799999999987\n",
      "12000 :  10.393799999999995\n",
      "13000 :  10.522999999999989\n",
      "14000 :  10.534699999999994\n",
      "15000 :  10.44569999999999\n",
      "16000 :  10.66369999999998\n",
      "17000 :  10.531799999999977\n",
      "18000 :  10.65569999999998\n",
      "19000 :  10.674500000000005\n",
      "20000 :  10.640699999999985\n",
      "21000 :  10.56389999999998\n",
      "22000 :  10.726199999999993\n",
      "23000 :  10.729099999999987\n",
      "24000 :  10.779399999999978\n",
      "25000 :  10.678299999999993\n",
      "26000 :  10.719299999999988\n",
      "27000 :  10.806499999999962\n",
      "28000 :  10.715099999999987\n",
      "29000 :  10.905499999999991\n",
      "30000 :  10.72289999999998\n"
     ]
    }
   ],
   "source": [
    "N0 = 1000\n",
    "agent = MC_agent(Environment(), N0)\n",
    "agent.train()\n",
    "agent.showing_and_storing_q_val()\n",
    "agent.accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Code To Watch The Agent Play The Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table=np.load('q_table_for_monti_carlo.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****EPISODE  1 *****\n",
      "\n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 6 \n",
      "\n",
      "You Took action in direction Right Ward  ➜ \n",
      "You are moving  6 -> 8 \n",
      "\n",
      "****You reached the goal!****\n",
      "Total Reward  of EPISODE  1  is  10.9 \n",
      "\n",
      "*****EPISODE  2 *****\n",
      "\n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 3 \n",
      "\n",
      "You Took action in direction Upward Ward 🠕\n",
      "You are moving  3 -> 6 \n",
      "\n",
      "You Took action in direction Right Ward  ➜ \n",
      "You are moving  6 -> 7 \n",
      "\n",
      "You Took action in direction Left Ward 🠔\n",
      "You are moving  7 -> 6 \n",
      "\n",
      "You Took action in direction Right Ward  ➜ \n",
      "You are moving  6 -> 8 \n",
      "\n",
      "****You reached the goal!****\n",
      "Total Reward  of EPISODE  2  is  11.5 \n",
      "\n",
      "*****EPISODE  3 *****\n",
      "\n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 6 \n",
      "\n",
      "You Took action in direction Right Ward  ➜ \n",
      "You are moving  6 -> 7 \n",
      "\n",
      "You Took action in direction Left Ward 🠔\n",
      "You are moving  7 -> 8 \n",
      "\n",
      "****You reached the goal!****\n",
      "Total Reward  of EPISODE  3  is  10.8 \n",
      "\n",
      "*****EPISODE  4 *****\n",
      "\n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 3 \n",
      "\n",
      "You Took action in direction Upward Ward 🠕\n",
      "You are moving  3 -> 0 \n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 3 \n",
      "\n",
      "You Took action in direction Upward Ward 🠕\n",
      "You are moving  3 -> 0 \n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 3 \n",
      "\n",
      "You Took action in direction Upward Ward 🠕\n",
      "You are moving  3 -> 0 \n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 3 \n",
      "\n",
      "You Took action in direction Upward Ward 🠕\n",
      "You are moving  3 -> 6 \n",
      "\n",
      "You Took action in direction Right Ward  ➜ \n",
      "You are moving  6 -> 8 \n",
      "\n",
      "****You reached the goal!****\n",
      "Total Reward  of EPISODE  4  is  9.5 \n",
      "\n",
      "*****EPISODE  5 *****\n",
      "\n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 6 \n",
      "\n",
      "You Took action in direction Right Ward  ➜ \n",
      "You are moving  6 -> 8 \n",
      "\n",
      "****You reached the goal!****\n",
      "Total Reward  of EPISODE  5  is  10.9 \n",
      "\n",
      "*****EPISODE  6 *****\n",
      "\n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 3 \n",
      "\n",
      "You Took action in direction Upward Ward 🠕\n",
      "You are moving  3 -> 0 \n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 3 \n",
      "\n",
      "You Took action in direction Upward Ward 🠕\n",
      "You are moving  3 -> 6 \n",
      "\n",
      "You Took action in direction Right Ward  ➜ \n",
      "You are moving  6 -> 7 \n",
      "\n",
      "You Took action in direction Left Ward 🠔\n",
      "You are moving  7 -> 8 \n",
      "\n",
      "****You reached the goal!****\n",
      "Total Reward  of EPISODE  6  is  10.2 \n",
      "\n",
      "*****EPISODE  7 *****\n",
      "\n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 6 \n",
      "\n",
      "You Took action in direction Right Ward  ➜ \n",
      "You are moving  6 -> 7 \n",
      "\n",
      "You Took action in direction Left Ward 🠔\n",
      "You are moving  7 -> 8 \n",
      "\n",
      "****You reached the goal!****\n",
      "Total Reward  of EPISODE  7  is  10.8 \n",
      "\n",
      "*****EPISODE  8 *****\n",
      "\n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 3 \n",
      "\n",
      "You Took action in direction Upward Ward 🠕\n",
      "You are moving  3 -> 6 \n",
      "\n",
      "You Took action in direction Right Ward  ➜ \n",
      "You are moving  6 -> 8 \n",
      "\n",
      "****You reached the goal!****\n",
      "Total Reward  of EPISODE  8  is  10.7 \n",
      "\n",
      "*****EPISODE  9 *****\n",
      "\n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 3 \n",
      "\n",
      "You Took action in direction Upward Ward 🠕\n",
      "You are moving  3 -> 0 \n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 3 \n",
      "\n",
      "You Took action in direction Upward Ward 🠕\n",
      "You are moving  3 -> 0 \n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 3 \n",
      "\n",
      "You Took action in direction Upward Ward 🠕\n",
      "You are moving  3 -> 0 \n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 3 \n",
      "\n",
      "You Took action in direction Upward Ward 🠕\n",
      "You are moving  3 -> 0 \n",
      "\n",
      "You Took action in direction Down Ward 🠗 \n",
      "You are moving  0 -> 3 \n",
      "\n",
      "You Took action in direction Upward Ward 🠕\n",
      "You are moving  3 -> 6 \n",
      "\n",
      "You Took action in direction Right Ward  ➜ \n",
      "You are moving  6 -> 8 \n",
      "\n",
      "****You reached the goal!****\n",
      "Total Reward  of EPISODE  9  is  9.1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env= Environment()\n",
    "for episode in range(9):\n",
    "    Total_reward=0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\")\n",
    "    while True:        \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done= env.step(action)\n",
    "        if(action==0):\n",
    "            print(\"You Took action in direction Left Ward 🠔\")\n",
    "            \n",
    "        elif(action==1):\n",
    "            print(\"You Took action in direction Right Ward  ➜ \")\n",
    "            \n",
    "        elif(action==3):\n",
    "            print(\"You Took action in direction Upward Ward 🠕\")\n",
    "            \n",
    "        else:\n",
    "            print(\"You Took action in direction Down Ward 🠗 \")\n",
    "            \n",
    "            \n",
    "        print(\"You are moving \",state,\"->\",new_state,\"\\n\")\n",
    "        \n",
    "        Total_reward=Total_reward+reward\n",
    "        if done:\n",
    "            if new_state == 8:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                print(\"Total Reward  of EPISODE \",episode+1,\" is \",Total_reward,\"\\n\")\n",
    "\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                print(\"Total Reward  of EPISODE \",episode+1,\" is \",Total_reward,\"\\n\")\n",
    "\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-D( lambada) Control Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa_Agent:\n",
    "    \n",
    "    def __init__(self, environment, n0, mlambda):\n",
    "        self.num_episodes=10000\n",
    "        self.max_steps_per_episode = 1000\n",
    "        self.n0 = float(n0)\n",
    "        self.env = environment\n",
    "        self.mlambda = mlambda\n",
    "        \n",
    "        # N(s) is the number of times that state s has been visited\n",
    "        # N(s,a) is the number of times that action a has been selected from state s.\n",
    "        self.N = np.zeros((self.env.observation_space,\n",
    "                           self.env.action_space))\n",
    "        \n",
    "        self.Q = np.zeros((self.env.observation_space,\n",
    "                           self.env.action_space))\n",
    "        \n",
    "        self.rewards_all_episodes = []\n",
    "        \n",
    "\n",
    "    # get optimal action, with epsilon exploration (epsilon dependent on number of visits to the state)\n",
    "    # ε-greedy exploration strategy with εt = N0/(N0 + N(st)), \n",
    "    def train_get_action(self, state):\n",
    "\n",
    "        n_visits = sum(self.N[state, :])       \n",
    "\n",
    "        # epsilon = N0/(N0 + N(st)\n",
    "        curr_epsilon = self.n0 / (self.n0 + n_visits)\n",
    "        \n",
    "        # epsilon greedy policy\n",
    "        if np.random.uniform(0, 1) < curr_epsilon:\n",
    "            action = self.env.sample()\n",
    "            return action\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state, :])\n",
    "            return action\n",
    "\n",
    "\n",
    "    def train(self):        \n",
    "        \n",
    "        # Loop episodes\n",
    "        for episode in range(self.num_episodes):\n",
    "            \n",
    "            self.E =np.zeros((self.env.observation_space, self.env.action_space))\n",
    "            \n",
    "            self.current_reward=0\n",
    "            # get initial state for current episode\n",
    "            state = self.env.reset()\n",
    "            action = self.train_get_action(state)\n",
    "            action_next = action\n",
    "            done=False\n",
    "            # Execute until game ends\n",
    "            for step in range(self.max_steps_per_episode):\n",
    "                # update visits\n",
    "                \n",
    "                self.N[state, action] += 1\n",
    "                \n",
    "                # execute action\n",
    "                state_next, reward,done = self.env.step(action)\n",
    "                \n",
    "                self.current_reward+=reward\n",
    "                q_value = self.Q[state, action]\n",
    "                                \n",
    "                if not done:\n",
    "                    # choose next action with epsilon greedy policy\n",
    "                    action_next = self.train_get_action(state_next)\n",
    "                    \n",
    "                    next_q_value= self.Q[state_next, action_next]\n",
    "                    delta = reward + next_q_value - q_value\n",
    "                else:\n",
    "                    delta = reward - q_value\n",
    "\n",
    "                \n",
    "                self.E[state, action] += 1\n",
    "                alpha = 1.0  / (self.N[state, action])\n",
    "                \n",
    "                update = alpha * delta * self.E\n",
    "                self.Q += update\n",
    "                self.E *= self.mlambda\n",
    "\n",
    "                # reassign s and a\n",
    "                state = state_next\n",
    "                action =  action_next\n",
    "                \n",
    "                if(done==True):\n",
    "                    break\n",
    "            \n",
    "            self.rewards_all_episodes.append(self.current_reward) \n",
    "\n",
    "    def showing_and_storing_q_val(self):\n",
    "        \n",
    "        print(\"\\n\\n********Q-table********\\n\")\n",
    "        print(self.Q)\n",
    "        #Saving Q table\n",
    "        np.save('q_table_for_TD_learning',self.Q)\n",
    "        \n",
    "    def accuracy(self):\n",
    "        rewards_per_thousand_episodes = np.split(np.array(self.rewards_all_episodes),self.num_episodes/1000)\n",
    "        count = 1000\n",
    "\n",
    "        print(\"********Average reward per thousand episodes********\\n\")\n",
    "        for r in rewards_per_thousand_episodes:\n",
    "            print(count, \": \", str(sum(r/1000)))\n",
    "            count += 1000\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[ 1.28097074  1.33098898  6.22680004  4.50706475]\n",
      " [ 1.80182428  2.01026006  4.46802953  3.09025837]\n",
      " [ 2.79379203  2.11059438  0.11904762  0.81339713]\n",
      " [-4.44241854 -4.57758391  6.54363653  4.98188643]\n",
      " [-4.30485569 -4.22984822  3.87452398  4.52282215]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 8.08146896  8.8386799   2.93707527  3.03386537]\n",
      " [ 8.7084463   9.26930928  1.96371532  2.32199863]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  4.170899999999998\n",
      "2000 :  7.613400000000011\n",
      "3000 :  8.623200000000017\n",
      "4000 :  9.262700000000018\n",
      "5000 :  9.964800000000007\n",
      "6000 :  9.909200000000022\n",
      "7000 :  9.88610000000002\n",
      "8000 :  9.869400000000027\n",
      "9000 :  10.235099999999997\n",
      "10000 :  10.165900000000008\n"
     ]
    }
   ],
   "source": [
    "N0 = 1000\n",
    "mlanbda=0\n",
    "agent = Sarsa_Agent(Environment(), N0,mlanbda)\n",
    "agent.train()\n",
    "agent.showing_and_storing_q_val()\n",
    "# print(agent.rewards_all_episodes)\n",
    "agent.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
